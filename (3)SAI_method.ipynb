{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e0fb4-775d-4105-a6b0-a75a13558f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df0cccf-782a-4433-9a17-9d21dd3c42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "uni_pure = pd.read_csv('EM_universe.csv', index_col='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba161f-fb1e-42b1-8289-bbda0c93d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copy of uni_pure to use\n",
    "uni = uni_pure.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381f774-8cb3-4310-ab13-92042ecf82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to datetiem and sort \n",
    "uni.index = pd.to_datetime(uni.index)\n",
    "uni = uni.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a0f4d-7c28-4742-b6a6-f5a55d36c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwated returns\n",
    "uni = uni[['name','id','Alpha', 'Market Capitalisation', 'Book-to-Market Ratio', 'EBIT', 'Investment','Stock Price Volatility', 'Mean Return', 'ROA', 'ROE',\n",
    " 'SGI', 'Debt-to-Equity Ratio', 'Market Risk Factor Loading', 'SMB Factor Loading', 'HML Factor Loading', 'RMW Factor Loading', 'CMA Factor Loading', 'return_adjusted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbcc55-9f4d-43e7-afae-fbb296ec9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename return_adjusted\n",
    "uni.rename(columns = {'return_adjusted':'return'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bb53e-cd69-4534-a4dc-c84cfab709d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16b189-82fa-4694-94d5-5888a95a819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that ranks data into quintiles\n",
    "def rank_factors(info):\n",
    "    for item in info.columns.tolist():\n",
    "        string = item + \" Rank\" \n",
    "        info[string] = pd.qcut(info[item], 5, labels = [item + \"1\", item + \"2\", item + \"3\",\n",
    "\n",
    "        item + \"4\", item + \"5\"])\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcb7e2-0bd8-4103-8dfd-4dcf99cde471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that creates an array of lists of factors for each available asset\n",
    "def fpg_prep(info):\n",
    "    # Drop the columns that do not contain the rankings\n",
    "    state = info.drop([ 'Alpha',\n",
    " 'Market Capitalisation',\n",
    " 'Book-to-Market Ratio',\n",
    " 'EBIT',\n",
    " 'Investment',\n",
    " 'Stock Price Volatility',\n",
    " 'Mean Return',\n",
    " 'ROA',\n",
    " 'ROE',\n",
    " 'SGI',\n",
    " 'Debt-to-Equity Ratio',\n",
    " 'Market Risk Factor Loading',\n",
    " 'SMB Factor Loading',\n",
    " 'HML Factor Loading',\n",
    " 'RMW Factor Loading',\n",
    " 'CMA Factor Loading', 'return'], axis=1)\n",
    "    # Drop the returns ranked column and assign remaining info to new \n",
    "    new = state.drop(\"return Rank\", axis = 1)\n",
    "    # Reset the index of state and drop the names column\n",
    "    state = state.reset_index()\n",
    "    final = []\n",
    "    # For each row, append final with each row as an array of its own\n",
    "    for i in range(0,len(state)):\n",
    "        final.append(state.loc[i, state.columns[1:]].tolist())\n",
    "    # Return both final and new\n",
    "    return final , new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5885629-22a3-473c-b09e-e5b4cbb3aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that mines assocation and lift rules, the 5th quintile returns and the true/false df for the FPG algorithm\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "def rules(final, sup, conf):\n",
    "    # Preprocessing of input argument into true and false for each discretisation\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(final).transform(final)\n",
    "    # Eg is the true/false dataframe in this form for the fp growth algorithm\n",
    "    eg = pd.DataFrame(te_ary , columns=te.columns_)\n",
    "    # True/false column of the highest quintile of returns\n",
    "    high_returns = eg.return5\n",
    "    # Finding frequent items in data for a minimum support of 5%\n",
    "    freq_items = fpgrowth(eg, min_support=sup, use_colnames = True)\n",
    "    # Discover association and causal rules\n",
    "    asso_rules = association_rules(freq_items , metric=\"confidence\",min_threshold =conf)\n",
    "    lift_rules = association_rules(freq_items , metric=\"lift\", min_threshold=1.2)\n",
    "    # Return true\\false dataframe , high returns , and the found\n",
    "    return eg, high_returns , asso_rules , lift_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71d3517-0bb2-4a1e-9e96-9632b89eadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get high return rules from rule set\n",
    "def high_ret_rules(asso_rules): \n",
    "    factors = []\n",
    "# For each row, if row contains \"Ret5\", then add that row’s antecedents to the factors array\n",
    "    for index in asso_rules.index.tolist():\n",
    "        #if (list(asso_rules.loc[index, ’consequents’])[0] in [\"Ret5\"]): \n",
    "        if (set([\"return5\"]).issubset(set(list(asso_rules.loc[index, 'consequents'])))):\n",
    "            factors.append(list(asso_rules.loc[index, 'antecedents']))\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad18237-42e3-4829-8cde-e22f656802bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get ruls\n",
    "def get_rules(unique_asso): \n",
    "    associatons = []\n",
    "    for asso in unique_asso:\n",
    "        # Check if each antecedent is in the right format (list) \n",
    "        if (isinstance(asso, list) == False):\n",
    "            associatons.append([asso]) \n",
    "        else:\n",
    "            associatons.append(asso)\n",
    "    # Return list of lists\n",
    "    return associatons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa777b96-dee7-492c-a238-94cb1e6cee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get rules for the period, by using the above functions\n",
    "def rules_for_period(info, sup, conf):\n",
    "    info = rank_factors(info.iloc[:,2:])\n",
    "    final, new = fpg_prep(info)\n",
    "    eg, high_returns , asso_rules , lift_rules = rules(final, sup, conf)\n",
    "    factors = high_ret_rules(asso_rules)\n",
    "    info = info.drop([ 'Alpha',\n",
    " 'Market Capitalisation',\n",
    " 'Book-to-Market Ratio',\n",
    " 'EBIT',\n",
    " 'Investment',\n",
    " 'Stock Price Volatility',\n",
    " 'Mean Return',\n",
    " 'ROA',\n",
    " 'ROE',\n",
    " 'SGI',\n",
    " 'Debt-to-Equity Ratio',\n",
    " 'Market Risk Factor Loading',\n",
    " 'SMB Factor Loading',\n",
    " 'HML Factor Loading',\n",
    " 'RMW Factor Loading',\n",
    " 'CMA Factor Loading', 'return'], axis=1)\n",
    "    rules_set = get_rules(factors)\n",
    "    return list(np.unique(np.array(rules_set))), eg, high_returns, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0490024-a9ec-45ce-9bed-c92060806435",
   "metadata": {},
   "source": [
    "# Chi Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f587d-8280-43ae-83de-885f83bb6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get expected and actual frequency of a rule\n",
    "def expected_freq(info, rule):\n",
    "    # Get number of equites with top quintile returns\n",
    "    num_ret_ind = len(info[info['return Rank'] == 'return5'])\n",
    "    \n",
    "    # Calculate expected frequency\n",
    "    mask = info.isin(rule)\n",
    "    filtered = info[mask].dropna(axis = 0, how = 'all')\n",
    "    num_rule_ind = len(filtered)\n",
    "    data_len = len(info)\n",
    "    ef_ind = (num_ret_ind/data_len)*(num_rule_ind/data_len)*data_len\n",
    "    \n",
    "    # Calculate actual frequency\n",
    "    rule.append('return5')\n",
    "    mask = info.isin(rule)\n",
    "    filtered = info[mask]\n",
    "    filtered = filtered.dropna(thresh=len(rule))\n",
    "    actual_freq = len(filtered)\n",
    "    rule.pop(-1)\n",
    "    return actual_freq, ef_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b68c459-fd31-42d1-8b8c-8aa7a6bda072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of strings into list of lists, where each string becomes its own list\n",
    "def list_list(lis):\n",
    "    \n",
    "    list_of_lists = []\n",
    "    \n",
    "    for string in lis:\n",
    "        # Create a new list containing the current string\n",
    "        new_list = [string]\n",
    "        # Add the new list to the list_of_lists\n",
    "        list_of_lists.append(new_list)\n",
    "    \n",
    "    return list_of_lists\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d16e3-0bc2-46d2-84fa-606404cb18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique elements for a list\n",
    "def get_unique_elements(input_list):\n",
    "    unique_elements = []\n",
    "    for element in input_list:\n",
    "        if element not in unique_elements:\n",
    "            unique_elements.append(element)\n",
    "    return unique_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b55543-059e-4eb6-9da5-0797eccba2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Function to get rules that pass the chi-squared pruning\n",
    "def causal_chi(info, rules):\n",
    "    if len(rules) == 0:\n",
    "        return []\n",
    "    causal = []\n",
    "    chi_stats = []\n",
    "    # Append rules that have a significant Chi-stat\n",
    "    for rule in rules:        \n",
    "        if type(rule)==np.str_:\n",
    "            rule = [rule]\n",
    "        actual_freq, ef_ind = expected_freq(info, rule)\n",
    "        stat = ((actual_freq-ef_ind)**2)/ef_ind\n",
    "        dof = len(info['return Rank'].unique())-1\n",
    "        p = stats.chi2.cdf(stat, dof)\n",
    "        if p > 0.99:\n",
    "            causal.append(rule)\n",
    "            chi_stats.append(stat)\n",
    "            \n",
    "    # If no rules were pruned take the 3 with the maximum chi-squared stat\n",
    "    if rules == causal or list_list(rules) == causal:\n",
    "        causal2 = []\n",
    "        index_1 = chi_stats.index(max(chi_stats))\n",
    "        causal2.append(causal[index_1])\n",
    "        chi_stats[index_1] = 0\n",
    "        \n",
    "        index_2 = chi_stats.index(max(chi_stats))\n",
    "        causal2.append(causal[index_2])\n",
    "        chi_stats[index_2] = 0\n",
    "        \n",
    "        index_3 = chi_stats.index(max(chi_stats))\n",
    "        causal2.append(causal[index_3])\n",
    "        chi_stats[index_3] = 0\n",
    "        \n",
    "        print(index_1, index_2, index_3)\n",
    "        \n",
    "        return get_unique_elements(causal2)\n",
    "        \n",
    "        \n",
    "    return causal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa2928-c539-46c3-886e-862a3e6dff47",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ceb52-85ab-4a2a-b0d2-535f8dda34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade openai wandb\n",
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc4ba3-9dad-414e-8b2e-23ebd1fb22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2661c71-f565-43e0-bee6-74760e1ee3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for Open AI API, you need to add the key\n",
    "API_KEY = 'Put API key here'\n",
    "API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12e0913-ab45-4b29-8de4-bd0b08a87f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prompt gpt-3.5 turbo\n",
    "def generate_chat_completion(messages, model=\"gpt-3.5-turbo\", temperature=1, max_tokens=None):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "\n",
    "    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(data))\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daec38a-8ac4-4830-8d68-91d288688013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part rephrase rule into something the LLM will understand\n",
    "dic = {'1':'very low', '2':'low',\"3\":'medium', '4':'high','5':'very high'}\n",
    "def rephrase(rule):\n",
    "    quant = dic[rule[-1]]\n",
    "    factor = rule[:-1]\n",
    "    return quant, factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f989c484-86b8-4eba-baae-16055d8ecccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rules that survived the LLM pruning\n",
    "import time\n",
    "def causal_LLM(rules):\n",
    "    causal = []\n",
    "    for rule in rules: \n",
    "        # Creating the prompt for each rule\n",
    "        if type(rule)==np.str_:\n",
    "            rule = [rule]\n",
    "        if len(rule)==1:\n",
    "            quant, factor = rephrase(rule[0])\n",
    "            q = 'Does a ' + quant + ' quarterly ' + factor + ' significantly increase the probability of a high quarterly return for a given equity?'\n",
    "        else:\n",
    "            q = 'Does'\n",
    "            for i in rule[:-1]:\n",
    "                quant, factor = rephrase(i)\n",
    "                q = q + ' a ' + quant + ' quarterly ' + factor +','\n",
    "            quant, factor = rephrase(rule[-1])\n",
    "            q = q[:-1] + ' and a ' + quant + ' quarterly ' + factor + ' significantly increase the probability of a high quarterly return for a given equity?'\n",
    "        result = None\n",
    "        \n",
    "        # Ask the LLM\n",
    "        while result is None:\n",
    "            try:\n",
    "                messages = [{\"role\": \"system\", \"content\": \"you only give one word, yes or no answers, no other answer is acceptable\"},{\"role\": \"user\", \"content\": q}]\n",
    "                response_text = generate_chat_completion(messages)\n",
    "                result = 1\n",
    "            except:\n",
    "                 pass\n",
    "                 print('error')\n",
    "        # Append valid rules to causal set\n",
    "        if response_text[0:3]=='yes' or response_text[0:3]=='Yes':\n",
    "            causal.append(rule)\n",
    "\n",
    "    return causal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a612e-c0cd-4021-85b2-068fe43cc8da",
   "metadata": {},
   "source": [
    "# Odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1fb9e-2f1d-43c1-a210-7ddecf0b3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get fair dataset for a rule\n",
    "def get_fair_datasets(true_control, false_control):\n",
    "    true_match = []\n",
    "    false_match = []\n",
    "\n",
    "    inter = pd.merge(true_control, false_control, how='inner')\n",
    "    # Build the fair datasets\n",
    "    for i, row in inter.iterrows():\n",
    "        mask_false = (false_control == row).all(axis=1)\n",
    "        mask_true = (true_control == row).all(axis=1)\n",
    "        false_match.extend(false_control[mask_false].index.tolist())\n",
    "        true_match.extend(true_control[mask_true].index.tolist())\n",
    "\n",
    "    return true_match, false_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc555f3-7ddc-4788-840e-735981c3306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oddsratio_CI(exposure , non_exposure , returns):\n",
    "    # Count for number of times both exposure and non-exposure groups have the consequent\n",
    "    n11 = 0\n",
    "    # Count for number of times the exposure has the consequent and non- exposure groups does not\n",
    "    n12 = 0\n",
    "    # Count for number of times the exposure does not have the consequent and non-exposure groups does\n",
    "    n21 = 0\n",
    "    # Count for number of times both exposure and non-exposure groups do not have the consequent\n",
    "    n22 = 0\n",
    "    for i in range(len(exposure)):\n",
    "    # If both the exposure and non exposure groups have returns in the 5th quantile , increment n11 by one\n",
    "        if (returns.loc[exposure.index[i]] == True) and (returns.loc[ non_exposure.index[i]] == True):\n",
    "            n11 += 1\n",
    "        elif (returns.loc[exposure.index[i]] == True) and (returns.loc[non_exposure.index[i]] == False): \n",
    "            n12 += 1\n",
    "        elif (returns.loc[exposure.index[i]] == False) and (returns.loc[ non_exposure.index[i]] == True):\n",
    "            n21 += 1\n",
    "        elif (returns.loc[exposure.index[i]] == False) and (returns.loc[non_exposure.index[i]] == False): \n",
    "            n22 += 1\n",
    "    # To ensure that you are not dividing by 0, if n12 or n21 are zero, set them to one\n",
    "    if n21 == 0: \n",
    "        n21 = 1\n",
    "    if n12 == 0: \n",
    "        n12 = 1\n",
    "    # Calculate the odds ratio point estimate\n",
    "    odds_ratio = n12/n21\n",
    "    # Compute the lower and upper bounds of the odds ratio’s 80% conficence interval\n",
    "    lower_bound = np.exp(np.log(odds_ratio) - (1.96*np.sqrt((1/n12) + (1/n21 ))))\n",
    "    upper_bound = np.exp(np.log(odds_ratio) + (1.15*np.sqrt((1/n12) + (1/n21 ))))\n",
    "    return lower_bound , upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9f822-ce51-4c96-a52f-964b1beba34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that mines causal rules from established associations\n",
    "def get_causal_rules(eg, rules, returns): \n",
    "    # Array to store mined causal rules \n",
    "    causal_rules = []\n",
    "    back_up_rules = []\n",
    "    back_up_ratios = []\n",
    "    for rule in rules:\n",
    "        if type(rule)==np.str_:\n",
    "            rule = [rule]\n",
    "        #print(rule)\n",
    "    \n",
    "        # Otherwise , for each association antecedent , search rows for when antecedent and true and false\n",
    "        trues = list(np.ones(len(rule), dtype=bool)) \n",
    "        falses = list(np.zeros(len(rule), dtype=bool))\n",
    "        \n",
    "        true_indices = np.all(eg[rule].values == trues, axis=1)\n",
    "        false_indices = np.all(eg[rule].values == falses, axis=1)\n",
    "        true = eg[true_indices]\n",
    "        false = eg[false_indices]\n",
    "\n",
    "        # Remove the returns columns and columns with antecedants in question. Only the control variables remain\n",
    "        remove = [\"return1\", \"return2\", \"return3\", \"return4\", 'return5'] \n",
    "        for cond in rule:\n",
    "            remove.append(cond[:-1] + \"1\") \n",
    "            remove.append(cond[:-1] + \"2\") \n",
    "            remove.append(cond[:-1] + \"3\")\n",
    "            remove.append(cond[:-1] + \"4\")\n",
    "            remove.append(cond[:-1] + \"5\")\n",
    "\n",
    "        \n",
    "        true_control = true.drop(remove , axis = 1)\n",
    "        false_control = false.drop(remove , axis = 1)\n",
    "        # Drop duplicates from true and false control sets to ensure that there is at most one set of matching rows\n",
    "\n",
    "        true_control = true_control.drop_duplicates(subset = true_control.columns , keep='first')\n",
    "        false_control = false_control.drop_duplicates(subset = false_control.columns , keep='first')\n",
    "        \n",
    "\n",
    "        # Retrieve the date indices from the fair datasets\n",
    "        true_match , false_match = get_fair_datasets(true_control, false_control)\n",
    "        \n",
    "        \n",
    "        # Getting returns columns for the rows of the fair dataset\n",
    "        exposure_returns = returns[true_match]\n",
    "        non_exposure_returns = returns[false_match]\n",
    "\n",
    "        # Compute the bounds of the rule’s odd ratio confidence interval\n",
    "        lower, _ = get_oddsratio_CI(exposure_returns, non_exposure_returns , returns)\n",
    "\n",
    "        #print(lower)\n",
    "        if (lower > 1):  \n",
    "            causal_rules.append(rule)\n",
    "        else:\n",
    "            back_up_rules.append(rule)\n",
    "            back_up_ratios.append(lower)\n",
    "            \n",
    "    if len(causal_rules) == 0 and len(back_up_ratios)>0: \n",
    "        high_ratios = max(back_up_ratios)\n",
    "   \n",
    "        for i in range(len(back_up_rules)):\n",
    "            if back_up_ratios[i] == high_ratios:\n",
    "                causal_rules.append(back_up_rules[i])\n",
    "        \n",
    "    return causal_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0092d2-17b0-4a56-aa3a-5306d7f52414",
   "metadata": {
    "tags": []
   },
   "source": [
    "assoc_rules, eg, ret, info = rules_for_period(uni[uni.index =='2011-03-31'])\n",
    "start_time = time.time()\n",
    "odds1 = get_causal_rules(eg, assoc_rules, ret)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d74d4-34da-47dd-9d75-fd3b7faafab1",
   "metadata": {},
   "source": [
    "Now I have the causal pruning functions i need to make a simulation that finds them at every period "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc632f-ac64-477e-9f81-fca85f8e14f3",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced5407-7eaa-45b0-93c3-64bdbbdd09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that gets all association rules and applies causal pruning to them for a given dataset\n",
    "def get_all_rule_sets(period_data, sup, conf):\n",
    "    assoc_rules, eg, ret, info = rules_for_period(period_data, sup, conf)\n",
    "\n",
    "    chi = causal_chi(info, assoc_rules)\n",
    "    print(\"chi done\")\n",
    "\n",
    "    LMM = causal_LLM(assoc_rules)\n",
    "\n",
    "    print('LMM done')\n",
    "    odds = get_causal_rules(eg, assoc_rules, ret)\n",
    "\n",
    "    print('odds done')\n",
    "\n",
    "    return chi, LMM, odds, assoc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f071e8-0846-4bac-b2bf-66a4b5a09a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Makes the ensemble models from the causal rule set\n",
    "def combine_rules(rule_set1, rule_set2):\n",
    "    or_rules = get_unique_elements(rule_set1+rule_set2)\n",
    "    and_rules = [value for value in rule_set1 if value in rule_set2]\n",
    "    return or_rules, and_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310ecc8-82b0-4d16-a30b-6feaedd910d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the n most frequent items from a list\n",
    "def find_n_most_frequent_items(lst, N):\n",
    "    # Use Counter to count occurrences of each element in the list\n",
    "    random.shuffle(lst)\n",
    "    item_counts = Counter(lst)\n",
    "\n",
    "    # Get the N most common items as a list of tuples (item, count)\n",
    "    most_common_items = item_counts.most_common()\n",
    "    \n",
    "    N_ind = min((len(most_common_items)-1), (N-1))\n",
    "    \n",
    "    max_count = most_common_items[N_ind][1]\n",
    "    \n",
    "    result1 = [item[0] for item in most_common_items if item[1] > max_count]\n",
    "    \n",
    "    # Extract only the items from the tuples and return as a list\n",
    "    result2 = [item[0] for item in most_common_items if item[1] == max_count]\n",
    "    #print(result2)\n",
    "    return result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475eb96b-57c8-4e3f-9743-4d329c949fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, given a ruleset finds the picks the equities due to those rules\n",
    "def get_equities_data(rules, ranked_data, cur_data):\n",
    "    # Return benchmark if ruleset is empty\n",
    "    if len(rules)==0:\n",
    "        results = [(cur_data['return'].mean()+1), len(cur_data)]\n",
    "        return results\n",
    "    # Else pick equities that satisfy the max number of rules until portfolios size is 25% of universe\n",
    "    else:\n",
    "        ranked_data.reset_index()\n",
    "        stocks = []\n",
    "        number_of_rules = len(rules)\n",
    "        max_stocks = round(len(ranked_data)*0.25)\n",
    "        for rule in rules:\n",
    "            if type(rule)==np.str_:\n",
    "                rule = [rule]\n",
    "            mask = ranked_data.isin(rule)\n",
    "            filtered = ranked_data[mask]\n",
    "            filtered = filtered.dropna(thresh=len(rule))\n",
    "            stocks += filtered.index.tolist()\n",
    "        \n",
    "\n",
    "        stocks1, stocks2 = find_n_most_frequent_items(stocks,round(max_stocks))\n",
    "                \n",
    "        cur_data2 = cur_data[cur_data.index.isin(stocks1)]\n",
    "        cur_data3 = cur_data[cur_data.index.isin(stocks2)]\n",
    "        \n",
    "        if len(stocks1) > 0 and len(stocks2) > 0:\n",
    "            mean_returns = (cur_data2['return'].mean()+1)*(len(stocks1)/max_stocks) + (cur_data3['return'].mean()+1)*(1-(len(stocks1)/max_stocks))\n",
    "        elif len(stocks1) > 0 and len(stocks2) == 0:\n",
    "            mean_returns = (cur_data2['return'].mean()+1)\n",
    "        elif len(stocks1) == 0 and len(stocks2) > 0:\n",
    "            mean_returns = (cur_data3['return'].mean()+1)\n",
    "        \n",
    "\n",
    "            \n",
    "        results = [mean_returns, max_stocks]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb9b14-e928-4339-ab15-1c1ebe9b2187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function runs the simulation. Picking the equities for each period and calculating the results of investing with them \n",
    "def simulation(data, periods, sup, conf):\n",
    "    dates = data.index.unique()\n",
    "    column_names = ['assoc_rules','chi', 'LMM', 'odds', 'chi_or_LLM', 'chi_and_LLM','chi_or_odds', 'chi_and_odds', 'LLM_or_odds', 'LLM_and_odds', 'chi_or_odds_or_LLM', 'chi_and_odds_and_LLM', 'benchmark']\n",
    "    # Create an empty DataFrame with column names\n",
    "    returns_df = pd.DataFrame(columns=column_names)\n",
    "    size_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    # Itterate through dats\n",
    "    for i in range(5, len(dates)-periods):\n",
    "        start_time = time.time()\n",
    "        # Get rolling window data\n",
    "        window_data = data[data.index.isin(dates[i:i+periods+1])]\n",
    "\n",
    "        \n",
    "        # Get window data where more than periods/2 periods are available for each stock\n",
    "        window_data = window_data[window_data.groupby('name').name.transform('count')>((periods/2)+1)].copy()\n",
    "        # Get current period data\n",
    "        current_data = window_data[window_data.index == dates[i+periods]].set_index('id')\n",
    "        \n",
    "        # Drop current period data from window data\n",
    "        window_data = window_data[window_data.index.isin(dates[i:i+periods])]\n",
    "        # Get the ARM and CRM rulesets\n",
    "        chi, LLM, odds, assoc_rules = get_all_rule_sets(window_data, sup, conf)\n",
    "\n",
    "        \n",
    "        # Get the ensemble models\n",
    "        chi_or_LLM, chi_and_LLM = combine_rules(chi, LLM)\n",
    "        chi_or_odds, chi_and_odds = combine_rules(chi, odds)\n",
    "        LLM_or_odds, LLM_and_odds = combine_rules(LLM, odds)\n",
    "        chi_or_odds_or_LLM, _ = combine_rules(LLM_or_odds, chi)\n",
    "        _, chi_and_odds_and_LLM = combine_rules(LLM_and_odds, chi)\n",
    "    \n",
    "        \n",
    "        # Encode current data\n",
    "        current_data = current_data.iloc[:,1:]\n",
    "        info2 = rank_factors(current_data).drop([ 'Alpha','Market Capitalisation', 'Book-to-Market Ratio', 'EBIT', 'Investment', 'Stock Price Volatility', 'Mean Return', 'ROA', 'ROE', 'SGI', 'Debt-to-Equity Ratio',\n",
    "         'Market Risk Factor Loading', 'SMB Factor Loading', 'HML Factor Loading', 'RMW Factor Loading', 'CMA Factor Loading','return'], axis=1) \n",
    "        \n",
    "        # Get the returns of each rule set\n",
    "        chi_equities = get_equities_data(chi, info2, current_data)\n",
    "        LLM_equities = get_equities_data(LLM, info2, current_data)\n",
    "        odds_equities = get_equities_data(odds, info2, current_data)\n",
    "        chi_or_LLM_equities = get_equities_data(chi_or_LLM, info2, current_data)\n",
    "        chi_and_LLM_equities = get_equities_data(chi_and_LLM, info2, current_data)\n",
    "        chi_or_odds_equities = get_equities_data(chi_or_odds, info2, current_data)\n",
    "        chi_and_odds_equities = get_equities_data(chi_and_odds, info2, current_data)\n",
    "        LLM_or_odds_equities = get_equities_data(LLM_or_odds, info2, current_data)\n",
    "        LLM_and_odds_equities = get_equities_data(LLM_and_odds, info2, current_data)\n",
    "        chi_or_odds_or_LLM_equities = get_equities_data(chi_or_odds_or_LLM, info2, current_data)\n",
    "        chi_and_odds_and_LLM_equities = get_equities_data(chi_and_odds_and_LLM, info2, current_data)\n",
    "        assoc_rules_equities = get_equities_data(assoc_rules, info2, current_data)\n",
    "        \n",
    "        \n",
    "        # Store results in dataframe\n",
    "        returns_df.loc[dates[i+periods],:] = [assoc_rules_equities[0], chi_equities[0], LLM_equities[0], odds_equities[0], chi_or_LLM_equities[0], chi_and_LLM_equities[0],\n",
    "                                         chi_or_odds_equities[0], chi_and_odds_equities[0], LLM_or_odds_equities[0], LLM_and_odds_equities[0],\n",
    "                                         chi_or_odds_or_LLM_equities[0], chi_and_odds_and_LLM_equities[0], (current_data['return'].mean()+1)]\n",
    "        \n",
    "        size_df.loc[dates[i+periods],:] = [assoc_rules_equities[1], chi_equities[1], LLM_equities[1], odds_equities[1], chi_or_LLM_equities[1], chi_and_LLM_equities[1],\n",
    "                                         chi_or_odds_equities[1], chi_and_odds_equities[1], LLM_or_odds_equities[1], LLM_and_odds_equities[1],\n",
    "                                         chi_or_odds_or_LLM_equities[1], chi_and_odds_and_LLM_equities[1], len(current_data)]\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(assoc_rules)\n",
    "        print(chi)\n",
    "        print(elapsed_time)\n",
    "        print(dates[i+periods])\n",
    "    return returns_df, size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768a6de-f7a6-45ad-9750-35320347d562",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the simulation\n",
    "start_time = time.time()\n",
    "ret, siz = simulation(uni, 12, 0.05, 0.20)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f29ae6c-2790-4ed3-907d-f6b9aa015d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop results from simulation\n",
    "ret.index.name = 'date'\n",
    "siz.index.name = 'date'\n",
    "ret.to_csv('ret.csv',index=True)\n",
    "siz.to_csv('siz.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a07ad6-4acc-4b8c-843b-e3960310ecc7",
   "metadata": {},
   "source": [
    "# Target shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3764bf-9fea-4449-8a8f-416d3915887f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to shuffle the target variables of the data \n",
    "def target_shuffle(data):\n",
    "    random_state = np.random.randint(0, 100)\n",
    "    shuffle = data.sample(frac=1, random_state = random_state).reset_index()\n",
    "    data = data.reset_index() \n",
    "    data['return'] = shuffle['return']\n",
    "    data = data.set_index('date')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775aee5-c896-4065-8197-b1df30eafdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to run the simulation again but with target shuffling, this time only recording anual returns \n",
    "def target_shuffle_simulation(data, periods):\n",
    "    dates = data.index.unique()\n",
    "    column_names = ['assoc_rules','chi', 'LMM', 'odds', 'chi_or_LLM', 'chi_and_LLM','chi_or_odds', 'chi_and_odds', 'LLM_or_odds', 'LLM_and_odds', 'chi_or_odds_or_LLM', 'chi_and_odds_and_LLM']\n",
    "    # Create an empty DataFrame with column names\n",
    "    returns_df = pd.DataFrame(columns=column_names)\n",
    "    size_df = pd.DataFrame(columns=column_names)\n",
    "    \n",
    "    # Itterate through periods\n",
    "    for i in range(5, len(dates)-periods):\n",
    "        \n",
    "        # Get rolling window data\n",
    "        window_data = data[data.index.isin(dates[i:i+periods+1])]\n",
    "        \n",
    "        # Get window data where more than periods/2 periods are available for each stock\n",
    "        window_data = window_data[window_data.groupby('name').name.transform('count')==((periods/2)+1)].copy()\n",
    "        # Get current period data\n",
    "        current_data = window_data[window_data.index == dates[i+periods]].set_index('id')\n",
    "        # Drop current period data from window data\n",
    "        window_data = target_shuffle(window_data[window_data.index.isin(dates[i:i+periods])])\n",
    "        \n",
    "        # Get CRM and ARM rulesets for data\n",
    "        chi, LLM, odds,assoc_rules = get_all_rule_sets(window_data, 0.05, 0.2)\n",
    "        \n",
    "        # Get ensemble model sets\n",
    "        chi_or_LLM, chi_and_LLM = combine_rules(chi, LLM)\n",
    "        chi_or_odds, chi_and_odds = combine_rules(chi, odds)\n",
    "        LLM_or_odds, LLM_and_odds = combine_rules(LLM, odds)\n",
    "        chi_or_odds_or_LLM, _ = combine_rules(LLM_or_odds, chi)\n",
    "        _, chi_and_odds_and_LLM = combine_rules(LLM_and_odds, chi)\n",
    "        \n",
    "        # Encode current data\n",
    "        current_data = current_data.iloc[:,1:]\n",
    "        info2 = rank_factors(current_data).drop([ 'Alpha', 'Market Capitalisation', 'Book-to-Market Ratio', 'EBIT', 'Investment', 'Stock Price Volatility', 'Mean Return', 'ROA',\n",
    "         'ROE', 'SGI', 'Debt-to-Equity Ratio', 'Market Risk Factor Loading', 'SMB Factor Loading', 'HML Factor Loading', 'RMW Factor Loading', 'CMA Factor Loading', 'return'], axis=1) \n",
    "        \n",
    "        \n",
    "        # Get results for each ruleset\n",
    "        chi_equities = get_equities_data(chi, info2, current_data)\n",
    "        LLM_equities = get_equities_data(LLM, info2, current_data)\n",
    "        odds_equities = get_equities_data(odds, info2, current_data)\n",
    "        chi_or_LLM_equities = get_equities_data(chi_or_LLM, info2, current_data)\n",
    "        chi_and_LLM_equities = get_equities_data(chi_and_LLM, info2, current_data)\n",
    "        chi_or_odds_equities = get_equities_data(chi_or_odds, info2, current_data)\n",
    "        chi_and_odds_equities = get_equities_data(chi_and_odds, info2, current_data)\n",
    "        LLM_or_odds_equities = get_equities_data(LLM_or_odds, info2, current_data)\n",
    "        LLM_and_odds_equities = get_equities_data(LLM_and_odds, info2, current_data)\n",
    "        chi_or_odds_or_LLM_equities = get_equities_data(chi_or_odds_or_LLM, info2, current_data)\n",
    "        chi_and_odds_and_LLM_equities = get_equities_data(chi_and_odds_and_LLM, info2, current_data)\n",
    "        assoc_rules_equities = get_equities_data(assoc_rules, info2, current_data)\n",
    "        \n",
    "        # Store returns in dataframe\n",
    "        returns_df.loc[dates[i+periods],:] = [assoc_rules_equities[0], chi_equities[0], LLM_equities[0], odds_equities[0], chi_or_LLM_equities[0], chi_and_LLM_equities[0],\n",
    "                                         chi_or_odds_equities[0], chi_and_odds_equities[0], LLM_or_odds_equities[0], LLM_and_odds_equities[0],\n",
    "                                         chi_or_odds_or_LLM_equities[0], chi_and_odds_and_LLM_equities[0]]\n",
    "\n",
    "    return returns_df.prod(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf50c35-bea9-4f69-99b3-a2f5ba9c8a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the target shuffling simulation for as many times as the number in the for loop \n",
    "column_names = ['assoc_rules','chi', 'LMM', 'odds', 'chi_or_LLM', 'chi_and_LLM','chi_or_odds', 'chi_and_odds', 'LLM_or_odds', 'LLM_and_odds', 'chi_or_odds_or_LLM', 'chi_and_odds_and_LLM']\n",
    "# Create an empty DataFrame with column names\n",
    "target_shuffle_results = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for i in range(150):\n",
    "    target_shuffle_results.loc[len(target_shuffle_results.index)] = target_shuffle_simulation(uni, 12)\n",
    "    print(target_shuffle_results)\n",
    "    target_shuffle_results.to_csv('target_shuffle_results.csv',index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
